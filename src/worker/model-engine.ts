import { InferenceRequest } from '../types';
import { LlamaEngine } from './llama-engine';
import fs from 'fs';

export interface InferenceResult {
  text: string;
  promptTokens: number;
  completionTokens: number;
}

export class ModelEngine {
  private llamaEngine: LlamaEngine | null = null;
  private activeRequests = 0;
  private totalRequests = 0;
  private totalResponseTime = 0;
  private totalTokens = 0;
  private demoMode = true;

  async loadModel(modelPath: string): Promise<void> {
    if (!fs.existsSync(modelPath)) {
      throw new Error(`Model file not found: ${modelPath}`);
    }

    console.log(`üì¶ Loading model: ${modelPath}`);

    try {
      // Initialize llama.cpp engine
      this.llamaEngine = new LlamaEngine({
        modelPath,
        contextSize: parseInt(process.env.CONTEXT_LENGTH || '2048'),
        gpuLayers: parseInt(process.env.GPU_LAYERS || '0'),
      });

      await this.llamaEngine.loadModel();
      this.demoMode = false;
      
      console.log('‚úÖ Real model loaded successfully');
    } catch (error: any) {
      console.error('‚ùå Failed to load real model:', error.message);
      console.warn('‚ö†Ô∏è  Falling back to demo mode');
      this.llamaEngine = null;
      this.demoMode = true;
    }
  }

  async processInference(request: InferenceRequest): Promise<InferenceResult> {
    this.activeRequests++;
    this.totalRequests++;

    const startTime = Date.now();

    try {
      let result: InferenceResult;

      if (this.demoMode || !this.llamaEngine) {
        // Demo mode - generate mock response
        console.log('‚ö†Ô∏è  Using demo mode for inference');
        result = await this.generateDemoResponse(request);
      } else {
        // Real inference with llama.cpp
        result = await this.llamaEngine.processInference(request);
      }

      const responseTime = Date.now() - startTime;
      this.totalResponseTime += responseTime;
      this.totalTokens += result.completionTokens;

      return result;
    } finally {
      this.activeRequests--;
    }
  }

  private async generateDemoResponse(request: InferenceRequest): Promise<InferenceResult> {
    // Simulate processing time
    await new Promise(resolve => setTimeout(resolve, 500 + Math.random() * 1000));

    const prompt = request.messages.map(m => m.content).join('\n');
    const response = `[Demo Response] I received your message: "${prompt}". This is a simulated response from the AI cluster worker. In production, this would be generated by the actual LLM model loaded with node-llama-cpp.`;

    return {
      text: response,
      promptTokens: Math.floor(prompt.length / 4),
      completionTokens: Math.floor(response.length / 4),
    };
  }

  async unloadModel(): Promise<void> {
    if (this.llamaEngine) {
      await this.llamaEngine.unload();
      this.llamaEngine = null;
    }
    this.demoMode = true;
  }

  getActiveRequests(): number {
    return this.activeRequests;
  }

  getTotalRequests(): number {
    return this.totalRequests;
  }

  getAvgResponseTime(): number {
    return this.totalRequests > 0 ? this.totalResponseTime / this.totalRequests : 0;
  }

  getTokensPerSecond(): number {
    const totalTime = this.totalResponseTime / 1000; // Convert to seconds
    return totalTime > 0 ? this.totalTokens / totalTime : 0;
  }

  isInDemoMode(): boolean {
    return this.demoMode;
  }

  getModelInfo(): any {
    if (this.llamaEngine) {
      return this.llamaEngine.getModelInfo();
    }
    return {
      mode: 'demo',
      message: 'Running in demo mode - no real model loaded',
    };
  }
}
